{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "62\n",
      "59\n",
      "55\n",
      "57\n",
      "30\n",
      "RNN / Embed / Sent = None, 300, 300\n",
      "GloVe / Trainable Word Embeddings = True, False\n",
      "Build model...\n",
      "Vocab size = 42391\n",
      "Loading GloVe\n",
      "Total number of null word embeddings:\n",
      "4043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0815 15:23:10.577317 4703471040 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:177: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(600, activation=\"relu\", kernel_regularizer=<keras.reg...)`\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:183: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "W0815 15:23:10.840016 4703471040 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:192: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "W0815 15:23:10.957637 4703471040 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 42)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 42)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 42, 300)      12717300    input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 42, 300)      90300       embedding_2[0][0]                \n",
      "                                                                 embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 300)          0           time_distributed_2[0][0]         \n",
      "                                                                 time_distributed_2[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 300)          1200        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 300)          1200        lambda_2[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 600)          0           batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 600)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 600)          360600      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 600)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 600)          2400        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 600)          360600      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 600)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 600)          2400        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 600)          360600      batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 600)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 600)          2400        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            1803        batch_normalization_7[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 13,900,803\n",
      "Trainable params: 1,178,703\n",
      "Non-trainable params: 12,722,100\n",
      "__________________________________________________________________________________________________\n",
      "Training\n",
      "Train on 549367 samples, validate on 9842 samples\n",
      "Epoch 1/42\n",
      "549367/549367 [==============================] - 174s 316us/step - loss: 0.7934 - acc: 0.6618 - val_loss: 0.6187 - val_acc: 0.7590\n",
      "Epoch 2/42\n",
      "549367/549367 [==============================] - 169s 308us/step - loss: 0.6361 - acc: 0.7412 - val_loss: 0.5731 - val_acc: 0.7777\n",
      "Epoch 3/42\n",
      "549367/549367 [==============================] - 169s 307us/step - loss: 0.5948 - acc: 0.7630 - val_loss: 0.5380 - val_acc: 0.7939\n",
      "Epoch 4/42\n",
      "549367/549367 [==============================] - 162s 295us/step - loss: 0.5715 - acc: 0.7751 - val_loss: 0.5224 - val_acc: 0.8032\n",
      "Epoch 5/42\n",
      "549367/549367 [==============================] - 161s 292us/step - loss: 0.5557 - acc: 0.7832 - val_loss: 0.5142 - val_acc: 0.8027\n",
      "Epoch 6/42\n",
      "549367/549367 [==============================] - 162s 295us/step - loss: 0.5443 - acc: 0.7890 - val_loss: 0.4965 - val_acc: 0.8146\n",
      "Epoch 7/42\n",
      "549367/549367 [==============================] - 160s 292us/step - loss: 0.5353 - acc: 0.7945 - val_loss: 0.5138 - val_acc: 0.8083\n",
      "Epoch 8/42\n",
      "549367/549367 [==============================] - 153s 279us/step - loss: 0.5270 - acc: 0.7993 - val_loss: 0.4989 - val_acc: 0.8173\n",
      "Epoch 9/42\n",
      "549367/549367 [==============================] - 160s 291us/step - loss: 0.5192 - acc: 0.8031 - val_loss: 0.5140 - val_acc: 0.8083\n",
      "Epoch 10/42\n",
      "549367/549367 [==============================] - 164s 299us/step - loss: 0.5130 - acc: 0.8069 - val_loss: 0.4846 - val_acc: 0.8237\n",
      "Epoch 11/42\n",
      "549367/549367 [==============================] - 159s 290us/step - loss: 0.5092 - acc: 0.8088 - val_loss: 0.4856 - val_acc: 0.8222\n",
      "Epoch 12/42\n",
      "549367/549367 [==============================] - 162s 294us/step - loss: 0.5042 - acc: 0.8119 - val_loss: 0.4847 - val_acc: 0.8244\n",
      "Epoch 13/42\n",
      "549367/549367 [==============================] - 160s 292us/step - loss: 0.5003 - acc: 0.8139 - val_loss: 0.4956 - val_acc: 0.8244\n",
      "Epoch 14/42\n",
      "549367/549367 [==============================] - 169s 308us/step - loss: 0.4957 - acc: 0.8164 - val_loss: 0.4812 - val_acc: 0.8248\n",
      "Epoch 15/42\n",
      "549367/549367 [==============================] - 165s 301us/step - loss: 0.4933 - acc: 0.8178 - val_loss: 0.4959 - val_acc: 0.8240\n",
      "Epoch 16/42\n",
      "549367/549367 [==============================] - 167s 305us/step - loss: 0.4900 - acc: 0.8200 - val_loss: 0.4764 - val_acc: 0.8315\n",
      "Epoch 17/42\n",
      "549367/549367 [==============================] - 162s 295us/step - loss: 0.4878 - acc: 0.8214 - val_loss: 0.4803 - val_acc: 0.8306\n",
      "Epoch 18/42\n",
      "549367/549367 [==============================] - 164s 299us/step - loss: 0.4850 - acc: 0.8233 - val_loss: 0.4809 - val_acc: 0.8298\n",
      "Epoch 19/42\n",
      "549367/549367 [==============================] - 170s 309us/step - loss: 0.4816 - acc: 0.8255 - val_loss: 0.4769 - val_acc: 0.8330\n",
      "Epoch 20/42\n",
      "549367/549367 [==============================] - 173s 315us/step - loss: 0.4813 - acc: 0.8254 - val_loss: 0.4844 - val_acc: 0.8294\n",
      "9824/9824 [==============================] - 1s 134us/step\n",
      "Test loss / test accuracy = 0.4910 / 0.8215\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "'''\n",
    "300D Model - Train / Test (epochs)\n",
    "=-=-=\n",
    "Batch size = 512\n",
    "Fixed GloVe\n",
    "- 300D SumRNN + Translate + 3 MLP (1.2 million parameters) - 0.8315 / 0.8235 / 0.8249 (22 epochs)\n",
    "- 300D GRU + Translate + 3 MLP (1.7 million parameters) - 0.8431 / 0.8303 / 0.8233 (17 epochs)\n",
    "- 300D LSTM + Translate + 3 MLP (1.9 million parameters) - 0.8551 / 0.8286 / 0.8229 (23 epochs)\n",
    "Following Liu et al. 2016, I don't update the GloVe embeddings during training.\n",
    "Unlike Liu et al. 2016, I don't initialize out of vocabulary embeddings randomly and instead leave them zeroed.\n",
    "The jokingly named SumRNN (summation of word embeddings) is 10-11x faster than the GRU or LSTM.\n",
    "Original numbers for sum / LSTM from Bowman et al. '15 and Bowman et al. '16\n",
    "=-=-=\n",
    "100D Sum + GloVe - 0.793 / 0.753\n",
    "100D LSTM + GloVe - 0.848 / 0.776\n",
    "300D LSTM + GloVe - 0.839 / 0.806\n",
    "'''\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import merge, recurrent, Dense, Input, Dropout, TimeDistributed,concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def extract_tokens_from_binary_parse(parse):\n",
    "    return parse.replace('(', ' ').replace(')', ' ').replace('-LRB-', '(').replace('-RRB-', ')').split()\n",
    "\n",
    "def yield_examples(fn, skip_no_majority=True, limit=None):\n",
    "  for i, line in enumerate(open(fn)):\n",
    "    if limit and i > limit:\n",
    "      break\n",
    "    data = json.loads(line)\n",
    "    label = data['gold_label']\n",
    "    s1 = ' '.join(extract_tokens_from_binary_parse(data['sentence1_binary_parse']))\n",
    "    s2 = ' '.join(extract_tokens_from_binary_parse(data['sentence2_binary_parse']))\n",
    "    if skip_no_majority and label == '-':\n",
    "      continue\n",
    "    yield (label, s1, s2)\n",
    "\n",
    "def get_data(fn, limit=None):\n",
    "  raw_data = list(yield_examples(fn=fn, limit=limit))\n",
    "  left = [s1 for _, s1, s2 in raw_data]\n",
    "  right = [s2 for _, s1, s2 in raw_data]\n",
    "  print(max(len(x.split()) for x in left))\n",
    "  print(max(len(x.split()) for x in right))\n",
    "\n",
    "  LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
    "  Y = np.array([LABELS[l] for l, s1, s2 in raw_data])\n",
    "  Y = np_utils.to_categorical(Y, len(LABELS))\n",
    "\n",
    "  return left, right, Y\n",
    "\n",
    "training = get_data('snli_1.0/snli_1.0_train.jsonl')\n",
    "validation = get_data('snli_1.0/snli_1.0_dev.jsonl')\n",
    "test = get_data('snli_1.0/snli_1.0_test.jsonl')\n",
    "\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(training[0] + training[1])\n",
    "\n",
    "# Lowest index from the tokenizer is 1 - we need to include 0 in our vocab count\n",
    "VOCAB = len(tokenizer.word_counts) + 1\n",
    "LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
    "#RNN = recurrent.LSTM\n",
    "#RNN = lambda *args, **kwargs: Bidirectional(recurrent.LSTM(*args, **kwargs))\n",
    "#RNN = recurrent.GRU\n",
    "#RNN = lambda *args, **kwargs: Bidirectional(recurrent.GRU(*args, **kwargs))\n",
    "# Summation of word embeddings\n",
    "RNN = None\n",
    "LAYERS = 1\n",
    "USE_GLOVE = True\n",
    "TRAIN_EMBED = False\n",
    "EMBED_HIDDEN_SIZE = 300\n",
    "SENT_HIDDEN_SIZE = 300\n",
    "BATCH_SIZE = 512\n",
    "PATIENCE = 4 # 8\n",
    "MAX_EPOCHS = 42\n",
    "MAX_LEN = 42\n",
    "DP = 0.2\n",
    "L2 = 4e-6\n",
    "ACTIVATION = 'relu'\n",
    "OPTIMIZER = 'rmsprop'\n",
    "print('RNN / Embed / Sent = {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE))\n",
    "print('GloVe / Trainable Word Embeddings = {}, {}'.format(USE_GLOVE, TRAIN_EMBED))\n",
    "\n",
    "to_seq = lambda X: pad_sequences(tokenizer.texts_to_sequences(X), maxlen=MAX_LEN)\n",
    "prepare_data = lambda data: (to_seq(data[0]), to_seq(data[1]), data[2])\n",
    "\n",
    "training = prepare_data(training)\n",
    "validation = prepare_data(validation)\n",
    "test = prepare_data(test)\n",
    "\n",
    "print('Build model...')\n",
    "print('Vocab size =', VOCAB)\n",
    "\n",
    "GLOVE_STORE = 'precomputed_glove.weights'\n",
    "if USE_GLOVE:\n",
    "  if not os.path.exists(GLOVE_STORE + '.npy'):\n",
    "    print('Computing GloVe')\n",
    "  \n",
    "    embeddings_index = {}\n",
    "    f = open('nlp_adversarial_examples/glove.840B.300d.txt')\n",
    "    for line in f:\n",
    "      values = line.split(' ')\n",
    "      word = values[0]\n",
    "      coefs = np.asarray(values[1:], dtype='float32')\n",
    "      embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    # prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((VOCAB, EMBED_HIDDEN_SIZE))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "      else:\n",
    "        print('Missing from GloVe: {}'.format(word))\n",
    "  \n",
    "    np.save(GLOVE_STORE, embedding_matrix)\n",
    "\n",
    "  print('Loading GloVe')\n",
    "  embedding_matrix = np.load(GLOVE_STORE + '.npy')\n",
    "\n",
    "  print('Total number of null word embeddings:')\n",
    "  print(np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, weights=[embedding_matrix], input_length=MAX_LEN, trainable=TRAIN_EMBED)\n",
    "else:\n",
    "  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, input_length=MAX_LEN)\n",
    "\n",
    "rnn_kwargs = dict(output_dim=SENT_HIDDEN_SIZE, dropout_W=DP, dropout_U=DP)\n",
    "SumEmbeddings = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n",
    "\n",
    "translate = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION))\n",
    "\n",
    "premise = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "hypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "\n",
    "prem = embed(premise)\n",
    "hypo = embed(hypothesis)\n",
    "\n",
    "prem = translate(prem)\n",
    "hypo = translate(hypo)\n",
    "\n",
    "if RNN and LAYERS > 1:\n",
    "  for l in range(LAYERS - 1):\n",
    "    rnn = RNN(return_sequences=True, **rnn_kwargs)\n",
    "    prem = BatchNormalization()(rnn(prem))\n",
    "    hypo = BatchNormalization()(rnn(hypo))\n",
    "rnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\n",
    "prem = rnn(prem)\n",
    "hypo = rnn(hypo)\n",
    "prem = BatchNormalization()(prem)\n",
    "hypo = BatchNormalization()(hypo)\n",
    "\n",
    "joint = concatenate([prem, hypo])\n",
    "joint = Dropout(DP)(joint)\n",
    "for i in range(3):\n",
    "  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, W_regularizer=l2(L2) if L2 else None)(joint)\n",
    "  joint = Dropout(DP)(joint)\n",
    "  joint = BatchNormalization()(joint)\n",
    "\n",
    "pred = Dense(len(LABELS), activation='softmax')(joint)\n",
    "\n",
    "model = Model(input=[premise, hypothesis], output=pred)\n",
    "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Training')\n",
    "_, tmpfn = tempfile.mkstemp()\n",
    "# Save the best model during validation and bail out of training early if we're not improving\n",
    "callbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(tmpfn, save_best_only=True, save_weights_only=True)]\n",
    "model.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, nb_epoch=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n",
    "\n",
    "# Restore the best found model during validation\n",
    "model.load_weights(tmpfn)\n",
    "\n",
    "loss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"saved/snli.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "reverse_word_map[0] = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_map[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2text(x,m):\n",
    "    sentence = [m[i] for i in x]\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> The church has cracks in the ceiling .'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2text(test[1][0],reverse_word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-970b0567679a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
